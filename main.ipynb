{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col = \"id\")\n",
    "test = pd.read_csv('test.csv', index_col = \"id\")\n",
    "submission = pd.read_csv('sample_submission.csv', index_col = \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('major', inplace = True, axis = 1)\n",
    "test.drop('major', inplace = True, axis = 1)\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('country', inplace = True, axis = 1)\n",
    "test.drop('country', inplace = True, axis = 1)\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['race_arab'] = pd.to_numeric(train['race_arab'], errors='coerce').fillna(-1).astype('int')\n",
    "train['race_asian'] = pd.to_numeric(train['race_asian'], errors='coerce').fillna(-1).astype('int')\n",
    "train['race_black'] = pd.to_numeric(train['race_black'], errors='coerce').fillna(-1).astype('int')\n",
    "train['race_white'] = pd.to_numeric(train['race_white'], errors='coerce').fillna(-1).astype('int')\n",
    "\n",
    "test['race_arab'] = pd.to_numeric(test['race_arab'], errors='coerce').fillna(-1).astype('int')\n",
    "test['race_asian'] = pd.to_numeric(test['race_asian'], errors='coerce').fillna(-1).astype('int')\n",
    "test['race_black'] = pd.to_numeric(test['race_black'], errors='coerce').fillna(-1).astype('int')\n",
    "test['race_white'] = pd.to_numeric(test['race_white'], errors='coerce').fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"age\"] = train[\"age\"].where(train[\"age\"] <= 100)\n",
    "train[\"age\"].fillna(train[\"age\"].mean(), inplace = True)\n",
    "train[\"age\"] = train[\"age\"].astype('int')\n",
    "\n",
    "test[\"age\"] = test[\"age\"].where(test[\"age\"] <= 100)\n",
    "test[\"age\"].fillna(test[\"age\"].mean(), inplace = True)\n",
    "test[\"age\"] = test[\"age\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"familysize\"] = train[\"familysize\"].where(train[\"familysize\"] <= 10)\n",
    "train[\"familysize\"].fillna(train[\"familysize\"].mean(), inplace = True)\n",
    "train[\"familysize\"] = train[\"familysize\"].astype('int')\n",
    "\n",
    "test[\"familysize\"] = test[\"familysize\"].where(test[\"familysize\"] <= 10)\n",
    "test[\"familysize\"].fillna(test[\"familysize\"].mean(), inplace = True)\n",
    "test[\"familysize\"] = test[\"familysize\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_introelapse\"] = np.log1p(train[\"introelapse\"])\n",
    "train[\"log_testelapse\"] = np.log1p(train[\"testelapse\"])\n",
    "train[\"log_surveyelapse\"] = np.log1p(train[\"surveyelapse\"])\n",
    "\n",
    "test[\"log_introelapse\"] = np.log1p(test[\"introelapse\"])\n",
    "test[\"log_testelapse\"] = np.log1p(test[\"testelapse\"])\n",
    "test[\"log_surveyelapse\"] = np.log1p(test[\"surveyelapse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns\n",
    "cols = list(cols)\n",
    "cols.remove('nerdy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train[cols])\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = pd.DataFrame(data = train_scaled, index = train.index, columns = cols)\n",
    "test_scaled = pd.DataFrame(data = test_scaled, index = test.index, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled['nerdy'] = train['nerdy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacker(model, model_name, train_data, test_data, fold):\n",
    "    test_preds = np.zeros(test_data.shape[0])\n",
    "    train_preds = np.zeros(train_data.shape[0])\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=fold,random_state=48,shuffle=True)\n",
    "    rmse_list=[]\n",
    "    n=0\n",
    "    \n",
    "    for train_index, test_index in kf.split(train_data[cols],train_data['nerdy']):\n",
    "        \n",
    "        X_train, X_valid = train_data[cols].iloc[train_index], train_data[cols].iloc[test_index]\n",
    "        y_train, y_valid = train_data['nerdy'].iloc[train_index], train_data['nerdy'].iloc[test_index]\n",
    "        \n",
    "        if model_name == 'catb':\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], silent=True)\n",
    "        elif model_name == 'xgb' or model_name == 'lgbm':\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], early_stopping_rounds=100, eval_metric=\"rmse\", verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "    \n",
    "        test_preds += model.predict(test_data[cols])/kf.n_splits\n",
    "        train_preds += model.predict(train_data[cols])/kf.n_splits\n",
    "        \n",
    "        rmse_list.append(np.sqrt(rmse(y_valid, model.predict(X_valid))))\n",
    "        \n",
    "        print(f\"fold: {n+1}, rmse: {rmse_list[n]}\")\n",
    "        n+=1  \n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.2339166539836521\n",
      "fold: 2, rmse: 1.2305278768463954\n",
      "fold: 3, rmse: 1.2368949262945386\n",
      "fold: 4, rmse: 1.2449839319403155\n",
      "fold: 5, rmse: 1.238406602106878\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "linear_train, linear_test = Stacker(lr, 'lr', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.1069166396319328\n",
      "fold: 2, rmse: 1.1114805030856163\n",
      "fold: 3, rmse: 1.1060851173940727\n",
      "fold: 4, rmse: 1.1209112975226432\n",
      "fold: 5, rmse: 1.1149785481471555\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor()\n",
    "lgbm_train, lgbm_test = Stacker(lgbm, 'lgbm', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.0988870284646\n",
      "fold: 2, rmse: 1.1102350347071694\n",
      "fold: 3, rmse: 1.0989500532513725\n",
      "fold: 4, rmse: 1.1165941969287367\n",
      "fold: 5, rmse: 1.1026734363136652\n"
     ]
    }
   ],
   "source": [
    "catb = CatBoostRegressor(use_best_model=True, iterations=1000, eval_metric='RMSE')\n",
    "catb_train, catb_test = Stacker(catb, 'catb', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.11764668720475\n",
      "fold: 2, rmse: 1.143263560866686\n",
      "fold: 3, rmse: 1.1367534771295764\n",
      "fold: 4, rmse: 1.1526447832280633\n",
      "fold: 5, rmse: 1.1381122537635717\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000)\n",
    "xgb_train, xgb_test = Stacker(xgb, 'xgb', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.1684038316250056\n",
      "fold: 2, rmse: 1.1717675498589148\n",
      "fold: 3, rmse: 1.1643791908208367\n",
      "fold: 4, rmse: 1.1778217716859596\n",
      "fold: 5, rmse: 1.1579661584814673\n"
     ]
    }
   ],
   "source": [
    "svmac = svm.SVR()\n",
    "svmac_train, svmac_test = Stacker(svmac, 'svr', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.112549149257852\n",
      "fold: 2, rmse: 1.133327401070487\n",
      "fold: 3, rmse: 1.1323174593800627\n",
      "fold: 4, rmse: 1.153019226960067\n",
      "fold: 5, rmse: 1.12936587673908\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestRegressor()\n",
    "forest_train, forest_test = Stacker(forest, 'forest', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 1.3018681247041817\n",
      "fold: 2, rmse: 1.379108481399514\n",
      "fold: 3, rmse: 1.3645429041438752\n",
      "fold: 4, rmse: 1.400085511576865\n",
      "fold: 5, rmse: 1.3464796042863911\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostRegressor()\n",
    "adaboost_train, adaboost_test = Stacker(adaboost, 'adaboost', train_scaled, test_scaled, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = np.concatenate((catb_train.reshape(-1,1), lgbm_train.reshape(-1,1), xgb_train.reshape(-1,1), svmac_train.reshape(-1,1), forest_train.reshape(-1,1)), axis = 1)\n",
    "stack_test = np.concatenate((catb_test.reshape(-1,1), lgbm_test.reshape(-1,1), xgb_test.reshape(-1,1), svmac_test.reshape(-1,1), forest_test.reshape(-1,1)), axis = 1)\n",
    "\n",
    "stack_train = pd.DataFrame(stack_train, columns = ['catb', 'lgbm', 'xgb', 'svmac', 'forest'])\n",
    "stack_test = pd.DataFrame(stack_test, columns = ['catb', 'lgbm', 'xgb', 'svmac', 'forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train.index = train_scaled.index\n",
    "stack_test.index = test_scaled.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = SelectKBest(f_regression)\n",
    "train_scaled_best = fs.fit_transform(train_scaled[cols], train_scaled['nerdy'])\n",
    "test_scaled_best = fs.transform(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled_best = pd.DataFrame(train_scaled_best, columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10'])\n",
    "test_scaled_best = pd.DataFrame(test_scaled_best, columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled_best.index = train_scaled.index\n",
    "test_scaled_best.index = test_scaled.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = pd.concat([train_scaled_best, stack_train], axis=1, join='inner')\n",
    "stack_test = pd.concat([test_scaled_best, stack_test], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, rmse: 0.1465482535224751\n",
      "fold: 2, rmse: 0.14912840310213507\n",
      "fold: 3, rmse: 0.16972436756892342\n",
      "fold: 4, rmse: 0.1478657566430714\n",
      "fold: 5, rmse: 0.15527702381714326\n"
     ]
    }
   ],
   "source": [
    "y = train['nerdy'].copy()\n",
    "\n",
    "catb = CatBoostRegressor(use_best_model=True, iterations=1000, eval_metric='RMSE')\n",
    "\n",
    "train_preds = np.zeros(stack_train.shape[0])\n",
    "test_preds = np.zeros(stack_test.shape[0])\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\n",
    "rmse_list=[]\n",
    "\n",
    "n=0\n",
    "for train_index, test_index in kf.split(stack_train, y):\n",
    "    \n",
    "    X_train, X_valid = stack_train.iloc[train_index], stack_train.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    catb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], silent=True)\n",
    "    \n",
    "    train_preds += catb.predict(stack_train)/kf.n_splits\n",
    "    test_preds += catb.predict(stack_test)/kf.n_splits\n",
    "    \n",
    "    rmse_list.append(np.sqrt(rmse(y_valid, catb.predict(X_valid))))\n",
    "    \n",
    "    print(f\"fold: {n+1}, rmse: {rmse_list[n]}\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['nerdy'] = test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bfac240a021757cbfb48e9bd473e4c53cbefae98bcf2a343923cb0bbe286595"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
